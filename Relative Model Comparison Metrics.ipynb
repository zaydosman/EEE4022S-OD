{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import cv2\n",
    "#import keras\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "from numpy import array, newaxis, expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'F:/School/UCT_4th_year/EEE4022S/Notebooks/thesis_practice/acfr_data/'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'testannot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "def rgb_to_categorical(mask):\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_mask = np.zeros((384,384))  \n",
    "    new_mask=new_mask.astype(int)\n",
    "    for i in range (384):\n",
    "        for j in range (384):\n",
    "            \n",
    "            if mask[i,j,0]==0 and mask[i,j,1]==0 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(0)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==64 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(1)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==128 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(2)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==128 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(3)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==0 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(4)\n",
    "            elif mask[i,j,0]==64 and mask[i,j,1]==0 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(5)\n",
    "            elif mask[i,j,0]==64 and mask[i,j,1]==64 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(6)\n",
    "            elif mask[i,j,0]==64 and mask[i,j,1]==64 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(7)\n",
    "            elif mask[i,j,0]==192 and mask[i,j,1]==192 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(8)\n",
    "            elif mask[i,j,0]==0 and mask[i,j,1]==0 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(9)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==64 and mask[i,j,2]==64:\n",
    "                new_mask[i,j]=int(10)  \n",
    "                \n",
    "    return new_mask\n",
    "\n",
    "def categorical_to_rgb(mask):\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_mask = np.zeros((384,384,3))  \n",
    "    new_mask=new_mask.astype(int)\n",
    "    thresh=0.35\n",
    "    for i in range (384):\n",
    "        for j in range (384):\n",
    "            \n",
    "            if mask[i,j,0]>=thresh:\n",
    "                new_mask[i,j,0]=int(0)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,1]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,2]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(128)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,3]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(128)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,4]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,5]>=thresh:\n",
    "                new_mask[i,j,0]=int(64)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,6]>=thresh:\n",
    "                new_mask[i,j,0]=int(64)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,7]>=thresh:\n",
    "                new_mask[i,j,0]=int(64)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,8]>=thresh:\n",
    "                new_mask[i,j,0]=int(192)\n",
    "                new_mask[i,j,1]=int(192)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,9]>=thresh:\n",
    "                new_mask[i,j,0]=int(0)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,0]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(64)\n",
    "                \n",
    "    return new_mask\n",
    "    \n",
    "# classes for data loading and preprocessing\n",
    "class Dataset:\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['undefined', 'ground', 'sky', 'vegetation', 'building', \n",
    "               'vehicle', 'human', 'fence', 'pole', \n",
    "               'cow', 'other']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i])\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image=cv2.resize(image,(384,384),interpolation=cv2.INTER_AREA)\n",
    "        mask=cv2.resize(mask,(384,384),interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        \n",
    "        mask=rgb_to_categorical(mask)        \n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        mask=mask.astype(np.float32)\n",
    "        \n",
    "         #add background if mask is not binary\n",
    "        #if mask.shape[-1] != 1:\n",
    "        #    background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "        #    mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            #datapoint=self.dataset[j]\n",
    "            #newdatapoint = np.expand_dims(datapoint,0)\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    return x.round().clip(0, 1)\n",
    "\n",
    "# define heavy augmentations\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        A.PadIfNeeded(min_height=384, min_width=384, always_apply=True, border_mode=0),\n",
    "        A.RandomCrop(height=384, width=384, always_apply=True),\n",
    "\n",
    "        A.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        A.IAAPerspective(p=0.5),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(p=1),\n",
    "                A.RandomBrightness(p=1),\n",
    "                A.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.IAASharpen(p=1),\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.RandomContrast(p=1),\n",
    "                A.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        A.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        A.PadIfNeeded(384, 384)\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for train images\n",
    "BACKBONE='mobilenetv2'\n",
    "BATCH_SIZE = 5\n",
    "CLASSES = ['undefined','ground', 'sky', 'vegetation', 'building', 'vehicle', 'human', 'fence', 'pole', 'cow', 'other']\n",
    "LR = 0.0001\n",
    "EPOCHS = 19\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES))  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    #augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug1=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug2=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug3=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug4=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug5=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug6=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug7=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug8=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataset_aug9=Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "# Dataset for validation images\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    #augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset_aug1 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset_aug2 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "valid_dataset_aug3 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset_aug4 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "valid_dataset_aug5 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug1 = Dataloder(train_dataset_aug1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug2 = Dataloder(train_dataset_aug2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug3 = Dataloder(train_dataset_aug3, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug4 = Dataloder(train_dataset_aug4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug5 = Dataloder(train_dataset_aug5, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug6 = Dataloder(train_dataset_aug6, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug7 = Dataloder(train_dataset_aug7, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug8 = Dataloder(train_dataset_aug8, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_aug9 = Dataloder(train_dataset_aug9, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug1 = Dataloder(valid_dataset_aug1, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug2 = Dataloder(valid_dataset_aug2, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug3 = Dataloder(valid_dataset_aug3, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug4 = Dataloder(valid_dataset_aug2, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# check shapes for errors\n",
    "#assert train_dataloader[0][0].shape == (BATCH_SIZE, 320, 320, 3)\n",
    "#assert train_dataloader[0][1].shape == (BATCH_SIZE, 320, 320, n_classes)\n",
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "    #DisplayCallback(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 1\n",
      "loading 2\n",
      "loading 3\n",
      "loading 4\n",
      "loading 5\n",
      "loading 6\n",
      "loading 7\n",
      "loading 8\n",
      "loading 9\n",
      "loading 10\n",
      "loading 1\n",
      "loading 2\n",
      "loading 3\n",
      "loading 4\n",
      "loading 5\n",
      "--- 3203.2667372226715 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#temp=np.array(train_dataloader)\n",
    "train_image_set_list=[]\n",
    "train_annot_set_list=[]\n",
    "for i in range (len(train_dataloader)):\n",
    "    train_image_set_list.extend(train_dataloader[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader[i][1])\n",
    "print('loading 1') \n",
    "for i in range (len(train_dataloader_aug1)):\n",
    "    train_image_set_list.extend(train_dataloader_aug1[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug1[i][1])\n",
    "print('loading 2')\n",
    "for i in range (len(train_dataloader_aug2)):\n",
    "    train_image_set_list.extend(train_dataloader_aug2[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug2[i][1])\n",
    "print('loading 3')    \n",
    "for i in range (len(train_dataloader_aug3)):\n",
    "    train_image_set_list.extend(train_dataloader_aug3[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug3[i][1])\n",
    "print('loading 4')\n",
    "for i in range (len(train_dataloader_aug4)):\n",
    "    train_image_set_list.extend(train_dataloader_aug4[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug4[i][1])\n",
    "print('loading 5')    \n",
    "for i in range (len(train_dataloader_aug5)):\n",
    "    train_image_set_list.extend(train_dataloader_aug5[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug5[i][1])\n",
    "print('loading 6')    \n",
    "for i in range (len(train_dataloader_aug6)):\n",
    "    train_image_set_list.extend(train_dataloader_aug6[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug6[i][1])\n",
    "print('loading 7')    \n",
    "for i in range (len(train_dataloader_aug7)):\n",
    "    train_image_set_list.extend(train_dataloader_aug7[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug7[i][1])\n",
    "print('loading 8')    \n",
    "for i in range (len(train_dataloader_aug8)):\n",
    "    train_image_set_list.extend(train_dataloader_aug8[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug8[i][1])\n",
    "print('loading 9')   \n",
    "for i in range (len(train_dataloader_aug9)):\n",
    "    train_image_set_list.extend(train_dataloader_aug9[i][0])\n",
    "    train_annot_set_list.extend(train_dataloader_aug9[i][1])\n",
    "print('loading 10')\n",
    "    \n",
    "train_image_set=np.array(train_image_set_list)\n",
    "train_annot_set=np.array(train_annot_set_list)    \n",
    "\n",
    "\n",
    "val_image_set_list=[]\n",
    "val_annot_set_list=[]\n",
    "for i in range (len(valid_dataloader)):\n",
    "    val_image_set_list.extend(valid_dataloader[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader[i][1])\n",
    "print('loading 1')\n",
    "for i in range (len(valid_dataloader_aug1)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug1[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug1[i][1])\n",
    "print('loading 2')\n",
    "for i in range (len(valid_dataloader_aug2)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug2[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug2[i][1])\n",
    "print('loading 3')\n",
    "for i in range (len(valid_dataloader_aug3)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug3[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug3[i][1])\n",
    "   \n",
    "print('loading 4')\n",
    "for i in range (len(valid_dataloader_aug4)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug4[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug4[i][1])\n",
    "   \n",
    "print('loading 5')\n",
    "\n",
    "val_annot_set=np.array(val_annot_set_list)\n",
    "val_image_set=np.array(val_image_set_list)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 384, 384, 3)\n",
      "(1050, 384, 384, 11)\n",
      "(75, 384, 384, 3)\n",
      "(75, 384, 384, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train_image_set.shape)\n",
    "print(train_annot_set.shape)\n",
    "print(val_image_set.shape)\n",
    "print(val_annot_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "10/10 [==============================] - 97s 7s/step - loss: 1.4127 - iou_score: 0.1318 - f1-score: 0.1491 - accuracy: 0.3342 - val_loss: 1.9687 - val_iou_score: 0.2495 - val_f1-score: 0.2766 - val_accuracy: 0.5180\n",
      "Epoch 2/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.6986 - iou_score: 0.3391 - f1-score: 0.3733 - accuracy: 0.5880 - val_loss: 3.8306 - val_iou_score: 0.2399 - val_f1-score: 0.2578 - val_accuracy: 0.4045\n",
      "Epoch 3/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.7177 - iou_score: 0.3576 - f1-score: 0.3934 - accuracy: 0.6341 - val_loss: 3.3518 - val_iou_score: 0.2372 - val_f1-score: 0.2527 - val_accuracy: 0.4002\n",
      "Epoch 4/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.7514 - iou_score: 0.3561 - f1-score: 0.3909 - accuracy: 0.6223 - val_loss: 3.8715 - val_iou_score: 0.2400 - val_f1-score: 0.2574 - val_accuracy: 0.4033\n",
      "Epoch 5/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.6950 - iou_score: 0.3513 - f1-score: 0.3830 - accuracy: 0.6974 - val_loss: 0.9674 - val_iou_score: 0.2051 - val_f1-score: 0.2406 - val_accuracy: 0.6332\n",
      "Epoch 6/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.6383 - iou_score: 0.3191 - f1-score: 0.3523 - accuracy: 0.6268 - val_loss: 2.3133 - val_iou_score: 0.1587 - val_f1-score: 0.1892 - val_accuracy: 0.3227\n",
      "Epoch 7/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.5866 - iou_score: 0.3415 - f1-score: 0.3748 - accuracy: 0.6034 - val_loss: 0.7127 - val_iou_score: 0.1810 - val_f1-score: 0.2040 - val_accuracy: 0.6975\n",
      "Epoch 8/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.5319 - iou_score: 0.3728 - f1-score: 0.4061 - accuracy: 0.5798 - val_loss: 0.7322 - val_iou_score: 0.2495 - val_f1-score: 0.2860 - val_accuracy: 0.6856\n",
      "Epoch 9/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.6053 - iou_score: 0.3856 - f1-score: 0.4211 - accuracy: 0.6960 - val_loss: 1.5680 - val_iou_score: 0.1539 - val_f1-score: 0.1789 - val_accuracy: 0.2727\n",
      "Epoch 10/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.6028 - iou_score: 0.3252 - f1-score: 0.3611 - accuracy: 0.6893 - val_loss: 1.4628 - val_iou_score: 0.2530 - val_f1-score: 0.2874 - val_accuracy: 0.2923\n",
      "Epoch 11/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.5289 - iou_score: 0.3564 - f1-score: 0.3907 - accuracy: 0.7111 - val_loss: 1.3299 - val_iou_score: 0.1882 - val_f1-score: 0.2184 - val_accuracy: 0.3264\n",
      "Epoch 12/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.5053 - iou_score: 0.3389 - f1-score: 0.3728 - accuracy: 0.6300 - val_loss: 1.8753 - val_iou_score: 0.2459 - val_f1-score: 0.2676 - val_accuracy: 0.3788\n",
      "Epoch 13/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.5775 - iou_score: 0.2950 - f1-score: 0.3313 - accuracy: 0.6468 - val_loss: 3.5449 - val_iou_score: 0.1962 - val_f1-score: 0.2113 - val_accuracy: 0.3568\n",
      "Epoch 14/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.6342 - iou_score: 0.3138 - f1-score: 0.3428 - accuracy: 0.6755 - val_loss: 2.4557 - val_iou_score: 0.1598 - val_f1-score: 0.1750 - val_accuracy: 0.3567\n",
      "Epoch 15/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.6298 - iou_score: 0.4020 - f1-score: 0.4332 - accuracy: 0.6493 - val_loss: 3.3173 - val_iou_score: 0.2328 - val_f1-score: 0.2479 - val_accuracy: 0.3567\n",
      "Epoch 16/19\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.5758 - iou_score: 0.4392 - f1-score: 0.4720 - accuracy: 0.7265 - val_loss: 1.9326 - val_iou_score: 0.2328 - val_f1-score: 0.2480 - val_accuracy: 0.3572\n",
      "Epoch 17/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.5376 - iou_score: 0.3756 - f1-score: 0.4117 - accuracy: 0.6744 - val_loss: 2.6243 - val_iou_score: 0.2325 - val_f1-score: 0.2476 - val_accuracy: 0.3570\n",
      "Epoch 18/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.5594 - iou_score: 0.3922 - f1-score: 0.4285 - accuracy: 0.6819 - val_loss: 1.9334 - val_iou_score: 0.2328 - val_f1-score: 0.2480 - val_accuracy: 0.3584\n",
      "Epoch 19/19\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.5501 - iou_score: 0.3132 - f1-score: 0.3462 - accuracy: 0.6917 - val_loss: 1.3161 - val_iou_score: 0.2361 - val_f1-score: 0.2527 - val_accuracy: 0.3720\n",
      "Epoch 1/19\n",
      "10/10 [==============================] - 8s 215ms/step - loss: 1.4760 - iou_score: 0.2444 - f1-score: 0.2595 - accuracy: 0.3801 - val_loss: 1.9417 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3615\n",
      "Epoch 2/19\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.6650 - iou_score: 0.3431 - f1-score: 0.3821 - accuracy: 0.5889 - val_loss: 1.9222 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3660\n",
      "Epoch 3/19\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.7264 - iou_score: 0.2680 - f1-score: 0.3038 - accuracy: 0.6404 - val_loss: 1.9485 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3735\n",
      "Epoch 4/19\n",
      "10/10 [==============================] - 1s 152ms/step - loss: 0.5169 - iou_score: 0.3291 - f1-score: 0.3639 - accuracy: 0.6439 - val_loss: 2.0477 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.1165\n",
      "Epoch 5/19\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.6023 - iou_score: 0.3017 - f1-score: 0.3445 - accuracy: 0.5321 - val_loss: 2.0704 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.1130\n",
      "Epoch 6/19\n",
      "10/10 [==============================] - 1s 152ms/step - loss: 0.6699 - iou_score: 0.3825 - f1-score: 0.4182 - accuracy: 0.6668 - val_loss: 1.9361 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.1199\n",
      "Epoch 7/19\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.5994 - iou_score: 0.3748 - f1-score: 0.4107 - accuracy: 0.6531 - val_loss: 1.7812 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.1277\n",
      "Epoch 8/19\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.5487 - iou_score: 0.3571 - f1-score: 0.3932 - accuracy: 0.6938 - val_loss: 1.6831 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3946\n",
      "Epoch 9/19\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.5491 - iou_score: 0.3827 - f1-score: 0.4181 - accuracy: 0.6310 - val_loss: 1.6955 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3552\n",
      "Epoch 10/19\n",
      "10/10 [==============================] - 1s 151ms/step - loss: 0.5743 - iou_score: 0.3505 - f1-score: 0.3862 - accuracy: 0.7414 - val_loss: 1.5256 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3668\n",
      "Epoch 11/19\n",
      "10/10 [==============================] - 1s 143ms/step - loss: 0.5520 - iou_score: 0.4769 - f1-score: 0.5137 - accuracy: 0.6045 - val_loss: 1.4930 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3570\n",
      "Epoch 12/19\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.5895 - iou_score: 0.3768 - f1-score: 0.4164 - accuracy: 0.6351 - val_loss: 1.4350 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3602\n",
      "Epoch 13/19\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.6287 - iou_score: 0.3210 - f1-score: 0.3581 - accuracy: 0.6939 - val_loss: 1.3506 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3960\n",
      "Epoch 14/19\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.5905 - iou_score: 0.4097 - f1-score: 0.4471 - accuracy: 0.6875 - val_loss: 1.3013 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.4003\n",
      "Epoch 15/19\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.5441 - iou_score: 0.4125 - f1-score: 0.4491 - accuracy: 0.6722 - val_loss: 1.3248 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3567\n",
      "Epoch 16/19\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.6034 - iou_score: 0.4043 - f1-score: 0.4417 - accuracy: 0.7019 - val_loss: 1.3082 - val_iou_score: 0.2000 - val_f1-score: 0.2000 - val_accuracy: 0.3570\n",
      "Epoch 17/19\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.4822 - iou_score: 0.3424 - f1-score: 0.3772 - accuracy: 0.6746 - val_loss: 1.2732 - val_iou_score: 0.2001 - val_f1-score: 0.2003 - val_accuracy: 0.3572\n",
      "Epoch 18/19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 145ms/step - loss: 0.5716 - iou_score: 0.3344 - f1-score: 0.3714 - accuracy: 0.6424 - val_loss: 1.2751 - val_iou_score: 0.2545 - val_f1-score: 0.2675 - val_accuracy: 0.3567\n",
      "Epoch 19/19\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.6729 - iou_score: 0.3787 - f1-score: 0.4171 - accuracy: 0.6826 - val_loss: 1.1605 - val_iou_score: 0.2358 - val_f1-score: 0.2512 - val_accuracy: 0.3707\n",
      "Epoch 1/19\n",
      "10/10 [==============================] - 6s 220ms/step - loss: 1.9428 - iou_score: 0.2078 - f1-score: 0.2176 - accuracy: 0.2149 - val_loss: 1.1399 - val_iou_score: 0.2070 - val_f1-score: 0.2130 - val_accuracy: 0.6803\n",
      "Epoch 2/19\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.6389 - iou_score: 0.3532 - f1-score: 0.3917 - accuracy: 0.5713 - val_loss: 1.1088 - val_iou_score: 0.2099 - val_f1-score: 0.2176 - val_accuracy: 0.6409\n",
      "Epoch 3/19\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.4541 - iou_score: 0.3456 - f1-score: 0.3844 - accuracy: 0.6460 - val_loss: 1.2746 - val_iou_score: 0.2004 - val_f1-score: 0.2007 - val_accuracy: 0.5346\n",
      "Epoch 4/19\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.6046 - iou_score: 0.3601 - f1-score: 0.3952 - accuracy: 0.7140 - val_loss: 1.2751 - val_iou_score: 0.2001 - val_f1-score: 0.2001 - val_accuracy: 0.7138\n",
      "Epoch 5/19\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.5355 - iou_score: 0.3717 - f1-score: 0.4092 - accuracy: 0.6826 - val_loss: 1.3179 - val_iou_score: 0.2001 - val_f1-score: 0.2002 - val_accuracy: 0.6552\n",
      "Epoch 6/19\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.6385 - iou_score: 0.3360 - f1-score: 0.3743 - accuracy: 0.6937 - val_loss: 1.3188 - val_iou_score: 0.2003 - val_f1-score: 0.2005 - val_accuracy: 0.7119\n",
      "Epoch 7/19\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.5134 - iou_score: 0.3315 - f1-score: 0.3707 - accuracy: 0.6394 - val_loss: 1.4279 - val_iou_score: 0.2002 - val_f1-score: 0.2003 - val_accuracy: 0.6810\n",
      "Epoch 8/19\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.5728 - iou_score: 0.4085 - f1-score: 0.4443 - accuracy: 0.7229 - val_loss: 1.2455 - val_iou_score: 0.2053 - val_f1-score: 0.2098 - val_accuracy: 0.7184\n",
      "Epoch 9/19\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.5398 - iou_score: 0.3385 - f1-score: 0.3777 - accuracy: 0.6106 - val_loss: 1.3381 - val_iou_score: 0.2011 - val_f1-score: 0.2021 - val_accuracy: 0.6904\n",
      "Epoch 10/19\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.5419 - iou_score: 0.3992 - f1-score: 0.4392 - accuracy: 0.6465 - val_loss: 1.2701 - val_iou_score: 0.2081 - val_f1-score: 0.2143 - val_accuracy: 0.6605\n",
      "Epoch 11/19\n",
      "10/10 [==============================] - 2s 156ms/step - loss: 0.5895 - iou_score: 0.4767 - f1-score: 0.5121 - accuracy: 0.6911 - val_loss: 0.9917 - val_iou_score: 0.2412 - val_f1-score: 0.2591 - val_accuracy: 0.7565\n",
      "Epoch 12/19\n",
      "10/10 [==============================] - 1s 153ms/step - loss: 0.5859 - iou_score: 0.3847 - f1-score: 0.4192 - accuracy: 0.7437 - val_loss: 0.8870 - val_iou_score: 0.2667 - val_f1-score: 0.2938 - val_accuracy: 0.7697\n",
      "Epoch 13/19\n",
      "10/10 [==============================] - 1s 152ms/step - loss: 0.5726 - iou_score: 0.4167 - f1-score: 0.4549 - accuracy: 0.6304 - val_loss: 0.9986 - val_iou_score: 0.2306 - val_f1-score: 0.2487 - val_accuracy: 0.7527\n",
      "Epoch 14/19\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.5752 - iou_score: 0.3646 - f1-score: 0.4063 - accuracy: 0.6098 - val_loss: 0.9709 - val_iou_score: 0.2409 - val_f1-score: 0.2623 - val_accuracy: 0.7480\n",
      "Epoch 15/19\n",
      "10/10 [==============================] - 2s 153ms/step - loss: 0.5372 - iou_score: 0.3247 - f1-score: 0.3612 - accuracy: 0.6489 - val_loss: 0.8165 - val_iou_score: 0.2874 - val_f1-score: 0.3157 - val_accuracy: 0.7713\n",
      "Epoch 16/19\n",
      "10/10 [==============================] - 2s 158ms/step - loss: 0.5545 - iou_score: 0.3348 - f1-score: 0.3729 - accuracy: 0.6584 - val_loss: 0.7970 - val_iou_score: 0.2971 - val_f1-score: 0.3304 - val_accuracy: 0.7576\n",
      "Epoch 17/19\n",
      "10/10 [==============================] - 1s 153ms/step - loss: 0.5352 - iou_score: 0.3741 - f1-score: 0.4105 - accuracy: 0.5967 - val_loss: 0.7814 - val_iou_score: 0.2997 - val_f1-score: 0.3319 - val_accuracy: 0.7707\n",
      "Epoch 18/19\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.5486 - iou_score: 0.3928 - f1-score: 0.4260 - accuracy: 0.7014 - val_loss: 0.7683 - val_iou_score: 0.3110 - val_f1-score: 0.3489 - val_accuracy: 0.7473\n",
      "Epoch 19/19\n",
      "10/10 [==============================] - 1s 152ms/step - loss: 0.5020 - iou_score: 0.2741 - f1-score: 0.3097 - accuracy: 0.6289 - val_loss: 0.7915 - val_iou_score: 0.3092 - val_f1-score: 0.3437 - val_accuracy: 0.6884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\tf-n-gpu\\lib\\site-packages\\keras_applications\\mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "10/10 [==============================] - 6s 248ms/step - loss: 1.7002 - iou_score: 0.1962 - f1-score: 0.2058 - accuracy: 0.2758 - val_loss: 1.3143 - val_iou_score: 0.2192 - val_f1-score: 0.2335 - val_accuracy: 0.1994\n",
      "Epoch 2/19\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.8380 - iou_score: 0.3214 - f1-score: 0.3529 - accuracy: 0.5979 - val_loss: 1.0869 - val_iou_score: 0.2423 - val_f1-score: 0.2690 - val_accuracy: 0.3437\n",
      "Epoch 3/19\n",
      "10/10 [==============================] - 2s 185ms/step - loss: 0.6732 - iou_score: 0.3486 - f1-score: 0.3871 - accuracy: 0.6282 - val_loss: 1.0337 - val_iou_score: 0.2532 - val_f1-score: 0.2804 - val_accuracy: 0.4362\n",
      "Epoch 4/19\n",
      "10/10 [==============================] - 2s 178ms/step - loss: 0.6228 - iou_score: 0.3284 - f1-score: 0.3653 - accuracy: 0.6400 - val_loss: 0.8805 - val_iou_score: 0.2756 - val_f1-score: 0.3035 - val_accuracy: 0.7158\n",
      "Epoch 5/19\n",
      "10/10 [==============================] - 2s 181ms/step - loss: 0.6559 - iou_score: 0.2991 - f1-score: 0.3313 - accuracy: 0.6926 - val_loss: 0.8925 - val_iou_score: 0.2722 - val_f1-score: 0.2995 - val_accuracy: 0.7289\n",
      "Epoch 6/19\n",
      "10/10 [==============================] - 2s 178ms/step - loss: 0.6406 - iou_score: 0.3441 - f1-score: 0.3790 - accuracy: 0.6456 - val_loss: 0.9731 - val_iou_score: 0.2428 - val_f1-score: 0.2605 - val_accuracy: 0.7356\n",
      "Epoch 7/19\n",
      "10/10 [==============================] - 2s 177ms/step - loss: 0.6209 - iou_score: 0.3635 - f1-score: 0.4021 - accuracy: 0.6531 - val_loss: 0.9227 - val_iou_score: 0.2595 - val_f1-score: 0.2834 - val_accuracy: 0.7441\n",
      "Epoch 8/19\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.5902 - iou_score: 0.3109 - f1-score: 0.3496 - accuracy: 0.6155 - val_loss: 0.9269 - val_iou_score: 0.2613 - val_f1-score: 0.2836 - val_accuracy: 0.7367\n",
      "Epoch 9/19\n",
      "10/10 [==============================] - 2s 176ms/step - loss: 0.5866 - iou_score: 0.3254 - f1-score: 0.3620 - accuracy: 0.6026 - val_loss: 0.9080 - val_iou_score: 0.2677 - val_f1-score: 0.2891 - val_accuracy: 0.7265\n",
      "Epoch 10/19\n",
      "10/10 [==============================] - 2s 177ms/step - loss: 0.6377 - iou_score: 0.3617 - f1-score: 0.3982 - accuracy: 0.6154 - val_loss: 0.8593 - val_iou_score: 0.2794 - val_f1-score: 0.3013 - val_accuracy: 0.7151\n",
      "Epoch 11/19\n",
      "10/10 [==============================] - 2s 177ms/step - loss: 0.6145 - iou_score: 0.3392 - f1-score: 0.3778 - accuracy: 0.6520 - val_loss: 0.8028 - val_iou_score: 0.2870 - val_f1-score: 0.3092 - val_accuracy: 0.7244\n",
      "Epoch 12/19\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.4963 - iou_score: 0.3224 - f1-score: 0.3610 - accuracy: 0.6758 - val_loss: 0.7819 - val_iou_score: 0.2924 - val_f1-score: 0.3157 - val_accuracy: 0.7255\n",
      "Epoch 13/19\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.5013 - iou_score: 0.4277 - f1-score: 0.4640 - accuracy: 0.6697 - val_loss: 0.8436 - val_iou_score: 0.2792 - val_f1-score: 0.2992 - val_accuracy: 0.7169\n",
      "Epoch 14/19\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.5439 - iou_score: 0.3004 - f1-score: 0.3423 - accuracy: 0.5945 - val_loss: 0.7988 - val_iou_score: 0.2954 - val_f1-score: 0.3211 - val_accuracy: 0.7349\n",
      "Epoch 15/19\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.5797 - iou_score: 0.3729 - f1-score: 0.4114 - accuracy: 0.6081 - val_loss: 0.8222 - val_iou_score: 0.2858 - val_f1-score: 0.3055 - val_accuracy: 0.6906\n",
      "Epoch 16/19\n",
      "10/10 [==============================] - 2s 176ms/step - loss: 0.5748 - iou_score: 0.3335 - f1-score: 0.3714 - accuracy: 0.6406 - val_loss: 0.8333 - val_iou_score: 0.2791 - val_f1-score: 0.2945 - val_accuracy: 0.6398\n",
      "Epoch 17/19\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.5071 - iou_score: 0.3902 - f1-score: 0.4261 - accuracy: 0.6845 - val_loss: 0.7786 - val_iou_score: 0.2892 - val_f1-score: 0.3103 - val_accuracy: 0.6837\n",
      "Epoch 18/19\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.5452 - iou_score: 0.4536 - f1-score: 0.4930 - accuracy: 0.6033 - val_loss: 0.8175 - val_iou_score: 0.2809 - val_f1-score: 0.2990 - val_accuracy: 0.6493\n",
      "Epoch 19/19\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.6002 - iou_score: 0.3559 - f1-score: 0.3932 - accuracy: 0.6719 - val_loss: 0.8022 - val_iou_score: 0.2849 - val_f1-score: 0.3056 - val_accuracy: 0.6550\n",
      "Epoch 1/19\n",
      "10/10 [==============================] - 7s 283ms/step - loss: 1.5889 - iou_score: 0.1241 - f1-score: 0.1364 - accuracy: 0.2923 - val_loss: 0.8813 - val_iou_score: 0.2986 - val_f1-score: 0.3252 - val_accuracy: 0.6890\n",
      "Epoch 2/19\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.8087 - iou_score: 0.3904 - f1-score: 0.4261 - accuracy: 0.6193 - val_loss: 0.7811 - val_iou_score: 0.3158 - val_f1-score: 0.3387 - val_accuracy: 0.7023\n",
      "Epoch 3/19\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.6239 - iou_score: 0.3818 - f1-score: 0.4150 - accuracy: 0.7102 - val_loss: 0.7269 - val_iou_score: 0.3202 - val_f1-score: 0.3418 - val_accuracy: 0.7139\n",
      "Epoch 4/19\n",
      "10/10 [==============================] - 2s 208ms/step - loss: 0.6539 - iou_score: 0.3792 - f1-score: 0.4170 - accuracy: 0.6701 - val_loss: 0.7574 - val_iou_score: 0.3144 - val_f1-score: 0.3376 - val_accuracy: 0.7265\n",
      "Epoch 5/19\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.5758 - iou_score: 0.2804 - f1-score: 0.3196 - accuracy: 0.5623 - val_loss: 0.8323 - val_iou_score: 0.3039 - val_f1-score: 0.3277 - val_accuracy: 0.7246\n",
      "Epoch 6/19\n",
      "10/10 [==============================] - 2s 214ms/step - loss: 0.6671 - iou_score: 0.4001 - f1-score: 0.4368 - accuracy: 0.6559 - val_loss: 0.8708 - val_iou_score: 0.2898 - val_f1-score: 0.3140 - val_accuracy: 0.7104\n",
      "Epoch 7/19\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.6108 - iou_score: 0.3472 - f1-score: 0.3880 - accuracy: 0.6287 - val_loss: 0.8642 - val_iou_score: 0.2955 - val_f1-score: 0.3223 - val_accuracy: 0.7243\n",
      "Epoch 8/19\n",
      "10/10 [==============================] - 2s 215ms/step - loss: 0.6108 - iou_score: 0.2568 - f1-score: 0.2917 - accuracy: 0.6276 - val_loss: 0.8118 - val_iou_score: 0.3075 - val_f1-score: 0.3323 - val_accuracy: 0.7316\n",
      "Epoch 9/19\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.5241 - iou_score: 0.3151 - f1-score: 0.3536 - accuracy: 0.5959 - val_loss: 0.8463 - val_iou_score: 0.2919 - val_f1-score: 0.3171 - val_accuracy: 0.7394\n",
      "Epoch 10/19\n",
      "10/10 [==============================] - 2s 212ms/step - loss: 0.5886 - iou_score: 0.4124 - f1-score: 0.4532 - accuracy: 0.6301 - val_loss: 0.7951 - val_iou_score: 0.3027 - val_f1-score: 0.3269 - val_accuracy: 0.7377\n",
      "Epoch 11/19\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.5506 - iou_score: 0.3056 - f1-score: 0.3464 - accuracy: 0.5908 - val_loss: 0.7684 - val_iou_score: 0.3101 - val_f1-score: 0.3340 - val_accuracy: 0.7495\n",
      "Epoch 12/19\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.6544 - iou_score: 0.3898 - f1-score: 0.4250 - accuracy: 0.7117 - val_loss: 0.7076 - val_iou_score: 0.3213 - val_f1-score: 0.3449 - val_accuracy: 0.7612\n",
      "Epoch 13/19\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.5536 - iou_score: 0.3297 - f1-score: 0.3678 - accuracy: 0.6775 - val_loss: 0.6800 - val_iou_score: 0.3254 - val_f1-score: 0.3469 - val_accuracy: 0.7606\n",
      "Epoch 14/19\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.5077 - iou_score: 0.4012 - f1-score: 0.4345 - accuracy: 0.7041 - val_loss: 0.6767 - val_iou_score: 0.3280 - val_f1-score: 0.3524 - val_accuracy: 0.7707\n",
      "Epoch 15/19\n",
      "10/10 [==============================] - 2s 204ms/step - loss: 0.5721 - iou_score: 0.3441 - f1-score: 0.3822 - accuracy: 0.6394 - val_loss: 0.6564 - val_iou_score: 0.3321 - val_f1-score: 0.3542 - val_accuracy: 0.7669\n",
      "Epoch 16/19\n",
      "10/10 [==============================] - 2s 207ms/step - loss: 0.5279 - iou_score: 0.3851 - f1-score: 0.4200 - accuracy: 0.7367 - val_loss: 0.6215 - val_iou_score: 0.3373 - val_f1-score: 0.3587 - val_accuracy: 0.7641\n",
      "Epoch 17/19\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.6006 - iou_score: 0.3267 - f1-score: 0.3644 - accuracy: 0.6871 - val_loss: 0.5710 - val_iou_score: 0.3424 - val_f1-score: 0.3638 - val_accuracy: 0.7638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/19\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.5446 - iou_score: 0.4175 - f1-score: 0.4558 - accuracy: 0.6391 - val_loss: 0.6138 - val_iou_score: 0.3393 - val_f1-score: 0.3632 - val_accuracy: 0.7695\n",
      "Epoch 19/19\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.5852 - iou_score: 0.3380 - f1-score: 0.3766 - accuracy: 0.6289 - val_loss: 0.5888 - val_iou_score: 0.3400 - val_f1-score: 0.3603 - val_accuracy: 0.7610\n"
     ]
    }
   ],
   "source": [
    "BACKBONES= ['vgg16','resnet34','inceptionv3','mobilenetv2', 'efficientnetb0']\n",
    "for BACKBONE in BACKBONES:\n",
    "    PRE_TRAINED_WEIGHTS='imagenet'\n",
    "\n",
    "    BATCH_SIZE = 5\n",
    "    CLASSES = ['undefined','ground', 'sky', 'vegetation', 'building', 'vehicle', 'human', 'fence', 'pole', 'cow', 'other']\n",
    "    LR = 0.0001\n",
    "    EPOCHS = 19\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "    n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES))  # case for binary and multiclass segmentation\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "    #create model\n",
    "    model = sm.PSPNet(BACKBONE, encoder_weights = PRE_TRAINED_WEIGHTS, classes=n_classes) #encoder_freeze = True,\n",
    "\n",
    "\n",
    "    # define optomizer\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "    # Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "    # set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights=np.array([1,1,1,1,1,1,1,1,1,1,1])) \n",
    "    focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "    # actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "    # total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),'accuracy']\n",
    "\n",
    "    # compile keras model with defined optimozer, loss and metrics\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy()\n",
    "    model.compile(optim, loss, metrics)\n",
    "    \n",
    "    callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=train_image_set,#train_dataloader[0][0],\n",
    "        y=train_annot_set,#train_dataloader[0][1],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=10, \n",
    "        epochs=EPOCHS, \n",
    "        callbacks=callbacks, \n",
    "        validation_data=(val_image_set,val_annot_set), \n",
    "        validation_steps=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
