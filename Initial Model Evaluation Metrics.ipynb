{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import cv2\n",
    "#import keras\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "from numpy import array, newaxis, expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'F:/School/UCT_4th_year/EEE4022S/Notebooks/thesis_practice/acfr_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'testannot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "def rgb_to_categorical(mask):\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_mask = np.zeros((384,384))  \n",
    "    new_mask=new_mask.astype(int)\n",
    "    for i in range (384):\n",
    "        for j in range (384):\n",
    "            \n",
    "            if mask[i,j,0]==0 and mask[i,j,1]==0 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(0)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==64 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(1)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==128 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(2)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==128 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(3)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==0 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(4)\n",
    "            elif mask[i,j,0]==64 and mask[i,j,1]==0 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(5)\n",
    "            elif mask[i,j,0]==64 and mask[i,j,1]==64 and mask[i,j,2]==0:\n",
    "                new_mask[i,j]=int(6)\n",
    "            elif mask[i,j,0]==64 and mask[i,j,1]==64 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(7)\n",
    "            elif mask[i,j,0]==192 and mask[i,j,1]==192 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(8)\n",
    "            elif mask[i,j,0]==0 and mask[i,j,1]==0 and mask[i,j,2]==128:\n",
    "                new_mask[i,j]=int(9)\n",
    "            elif mask[i,j,0]==128 and mask[i,j,1]==64 and mask[i,j,2]==64:\n",
    "                new_mask[i,j]=int(10)  \n",
    "                \n",
    "    return new_mask\n",
    "\n",
    "def categorical_to_rgb(mask):\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_mask = np.zeros((384,384,3))  \n",
    "    new_mask=new_mask.astype(int)\n",
    "    thresh=0.35\n",
    "    for i in range (384):\n",
    "        for j in range (384):\n",
    "            \n",
    "            if mask[i,j,0]>=thresh:\n",
    "                new_mask[i,j,0]=int(0)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,1]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,2]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(128)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,3]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(128)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,4]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,5]>=thresh:\n",
    "                new_mask[i,j,0]=int(64)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,6]>=thresh:\n",
    "                new_mask[i,j,0]=int(64)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(0)\n",
    "            elif mask[i,j,7]>=thresh:\n",
    "                new_mask[i,j,0]=int(64)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,8]>=thresh:\n",
    "                new_mask[i,j,0]=int(192)\n",
    "                new_mask[i,j,1]=int(192)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,9]>=thresh:\n",
    "                new_mask[i,j,0]=int(0)\n",
    "                new_mask[i,j,1]=int(0)\n",
    "                new_mask[i,j,2]=int(128)\n",
    "            elif mask[i,j,0]>=thresh:\n",
    "                new_mask[i,j,0]=int(128)\n",
    "                new_mask[i,j,1]=int(64)\n",
    "                new_mask[i,j,2]=int(64)\n",
    "                \n",
    "    return new_mask\n",
    "    \n",
    "# classes for data loading and preprocessing\n",
    "class Dataset:\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['undefined', 'ground', 'sky', 'vegetation', 'building', \n",
    "               'vehicle', 'human', 'fence', 'pole', \n",
    "               'cow', 'other']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i])\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image=cv2.resize(image,(384,384),interpolation=cv2.INTER_AREA)\n",
    "        mask=cv2.resize(mask,(384,384),interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        \n",
    "        mask=rgb_to_categorical(mask)        \n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        mask=mask.astype(np.float32)\n",
    "        \n",
    "         #add background if mask is not binary\n",
    "        #if mask.shape[-1] != 1:\n",
    "        #    background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "        #    mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            #datapoint=self.dataset[j]\n",
    "            #newdatapoint = np.expand_dims(datapoint,0)\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    return x.round().clip(0, 1)\n",
    "\n",
    "# define heavy augmentations\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        A.PadIfNeeded(min_height=384, min_width=384, always_apply=True, border_mode=0),\n",
    "        A.RandomCrop(height=384, width=384, always_apply=True),\n",
    "\n",
    "        A.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        A.IAAPerspective(p=0.5),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(p=1),\n",
    "                A.RandomBrightness(p=1),\n",
    "                A.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.IAASharpen(p=1),\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.RandomContrast(p=1),\n",
    "                A.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        A.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        A.PadIfNeeded(384, 384)\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'mobilenetv2'\n",
    "\n",
    "CLASSES = ['undefined','ground', 'sky', 'vegetation', 'building', 'vehicle', 'human', 'fence', 'pole', 'cow', 'other']\n",
    "\n",
    "\n",
    "PRE_TRAINED_WEIGHTS='imagenet'\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "CLASSES = ['undefined','ground', 'sky', 'vegetation', 'building', 'vehicle', 'human', 'fence', 'pole', 'cow', 'other']\n",
    "LR = 0.0001\n",
    "EPOCHS = 19\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES))  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset for validation images\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    #augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset_aug1 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset_aug2 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "valid_dataset_aug3 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataset_aug4 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "valid_dataset_aug5 = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes=CLASSES, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocess_input),\n",
    ")\n",
    "\n",
    "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug1 = Dataloder(valid_dataset_aug1, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug2 = Dataloder(valid_dataset_aug2, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug3 = Dataloder(valid_dataset_aug3, batch_size=1, shuffle=False)\n",
    "valid_dataloader_aug4 = Dataloder(valid_dataset_aug2, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# check shapes for errors\n",
    "#assert train_dataloader[0][0].shape == (BATCH_SIZE, 320, 320, 3)\n",
    "#assert train_dataloader[0][1].shape == (BATCH_SIZE, 320, 320, n_classes)\n",
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "    #DisplayCallback(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 1\n",
      "loading 2\n",
      "loading 3\n",
      "loading 4\n",
      "loading 5\n",
      "--- 191.15102696418762 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "val_image_set_list=[]\n",
    "val_annot_set_list=[]\n",
    "for i in range (len(valid_dataloader)):\n",
    "    val_image_set_list.extend(valid_dataloader[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader[i][1])\n",
    "print('loading 1')\n",
    "for i in range (len(valid_dataloader_aug1)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug1[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug1[i][1])\n",
    "print('loading 2')\n",
    "for i in range (len(valid_dataloader_aug2)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug2[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug2[i][1])\n",
    "print('loading 3')\n",
    "for i in range (len(valid_dataloader_aug3)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug3[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug3[i][1])\n",
    "   \n",
    "print('loading 4')\n",
    "for i in range (len(valid_dataloader_aug4)):\n",
    "    val_image_set_list.extend(valid_dataloader_aug4[i][0])\n",
    "    val_annot_set_list.extend(valid_dataloader_aug4[i][1])\n",
    "   \n",
    "print('loading 5')\n",
    "\n",
    "val_annot_set=np.array(val_annot_set_list)\n",
    "val_image_set=np.array(val_image_set_list)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 384, 384, 3)\n",
      "(75, 384, 384, 11)\n"
     ]
    }
   ],
   "source": [
    "#print(train_image_set.shape)\n",
    "#print(train_annot_set.shape)\n",
    "#val_annot_set=np.expand_dims(val_annot_set, 0)\n",
    "#val_image_set=np.expand_dims(val_image_set, 0)\n",
    "#val_annot_set=tf.convert_to_tensor(val_annot_set)\n",
    "#val_image_set=tf.convert_to_tensor(val_image_set)\n",
    "\n",
    "#val_image_set=np.asarray(val_image_set).astype(np.float32)\n",
    "#val_annot_set=np.asarray(val_annot_set).astype(np.float32)\n",
    "#val_image_set=tf.squeeze(val_image_set)\n",
    "#val_annot_set=tf.squeeze(val_annot_set)\n",
    "#val_image_set=val_image_set.squeeze()\n",
    "#val_annot_set=val_annot_set.squeeze()\n",
    "\n",
    "print(val_image_set.shape)\n",
    "print(val_annot_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 141s 25s/step - loss: 8.9668 - iou_score: 0.0328 - f1-score: 0.0557 - accuracy: 0.0499\n",
      "3/3 [==============================] - 42s 12s/step - loss: 9.0605 - iou_score: 0.0333 - f1-score: 0.0567 - accuracy: 0.0467\n",
      "3/3 [==============================] - 42s 12s/step - loss: 9.1747 - iou_score: 0.0330 - f1-score: 0.0563 - accuracy: 0.0480\n",
      "3/3 [==============================] - 42s 12s/step - loss: 9.1827 - iou_score: 0.0330 - f1-score: 0.0562 - accuracy: 0.0480\n",
      "Loss: 9.1827\n",
      "--- 41.90893340110779 seconds ---\n",
      "3/3 [==============================] - 10s 1s/step - loss: 2.0431 - iou_score: 0.0475 - f1-score: 0.0776 - accuracy: 0.1162\n",
      "3/3 [==============================] - 2s 579ms/step - loss: 2.0380 - iou_score: 0.0475 - f1-score: 0.0778 - accuracy: 0.1176\n",
      "3/3 [==============================] - 2s 591ms/step - loss: 2.0380 - iou_score: 0.0475 - f1-score: 0.0778 - accuracy: 0.1176\n",
      "3/3 [==============================] - 2s 578ms/step - loss: 2.0380 - iou_score: 0.0475 - f1-score: 0.0778 - accuracy: 0.1176\n",
      "Loss: 2.038\n",
      "--- 2.084425926208496 seconds ---\n",
      "3/3 [==============================] - 13s 2s/step - loss: 2.1504 - iou_score: 0.0476 - f1-score: 0.0773 - accuracy: 0.0032\n",
      "3/3 [==============================] - 2s 437ms/step - loss: 2.1450 - iou_score: 0.0471 - f1-score: 0.0766 - accuracy: 0.0032\n",
      "3/3 [==============================] - 2s 435ms/step - loss: 2.1450 - iou_score: 0.0471 - f1-score: 0.0766 - accuracy: 0.0032\n",
      "3/3 [==============================] - 2s 435ms/step - loss: 2.1450 - iou_score: 0.0471 - f1-score: 0.0766 - accuracy: 0.0032\n",
      "Loss: 2.145\n",
      "--- 1.5837652683258057 seconds ---\n",
      "3/3 [==============================] - 6s 833ms/step - loss: 2.1891 - iou_score: 0.0766 - f1-score: 0.1188 - accuracy: 0.0395\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 2.1876 - iou_score: 0.0762 - f1-score: 0.1188 - accuracy: 0.0394\n",
      "3/3 [==============================] - 1s 315ms/step - loss: 2.1876 - iou_score: 0.0762 - f1-score: 0.1188 - accuracy: 0.0394\n",
      "3/3 [==============================] - 1s 313ms/step - loss: 2.1876 - iou_score: 0.0762 - f1-score: 0.1188 - accuracy: 0.0394\n",
      "Loss: 2.1876\n",
      "--- 1.175830364227295 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\tf-n-gpu\\lib\\site-packages\\keras_applications\\mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 5s 638ms/step - loss: 4.9593 - iou_score: 0.0663 - f1-score: 0.0990 - accuracy: 0.0036\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 4.9577 - iou_score: 0.0658 - f1-score: 0.0983 - accuracy: 0.0035\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 4.9577 - iou_score: 0.0658 - f1-score: 0.0983 - accuracy: 0.0035\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 4.9577 - iou_score: 0.0658 - f1-score: 0.0983 - accuracy: 0.0035\n",
      "Loss: 4.9577\n",
      "--- 0.9684114456176758 seconds ---\n",
      "3/3 [==============================] - 5s 521ms/step - loss: 2.8364 - iou_score: 0.0174 - f1-score: 0.0325 - accuracy: 0.0092\n",
      "3/3 [==============================] - 1s 300ms/step - loss: 2.8349 - iou_score: 0.0179 - f1-score: 0.0335 - accuracy: 0.0090\n",
      "3/3 [==============================] - 1s 303ms/step - loss: 2.8349 - iou_score: 0.0179 - f1-score: 0.0335 - accuracy: 0.0090\n",
      "3/3 [==============================] - 1s 302ms/step - loss: 2.8349 - iou_score: 0.0179 - f1-score: 0.0335 - accuracy: 0.0090\n",
      "Loss: 2.8349\n",
      "--- 1.121999979019165 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BACKBONES= ['vgg16','resnet152','densenet201','inceptionv3','mobilenetv2', 'efficientnetb0']\n",
    "for BACKBONE in BACKBONES:\n",
    "    PRE_TRAINED_WEIGHTS='imagenet'\n",
    "\n",
    "    BATCH_SIZE = 5\n",
    "    CLASSES = ['undefined','ground', 'sky', 'vegetation', 'building', 'vehicle', 'human', 'fence', 'pole', 'cow', 'other']\n",
    "    LR = 0.0001\n",
    "    EPOCHS = 20\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "    n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES))  # case for binary and multiclass segmentation\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "    #create model\n",
    "    model = sm.Unet(BACKBONE, encoder_weights = PRE_TRAINED_WEIGHTS, classes=n_classes) #encoder_freeze = True,\n",
    "\n",
    "\n",
    "    # define optomizer\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "    # Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "    # set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights=np.array([1,1,1,1,1,1,1,1,1,1,1])) \n",
    "    focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "    total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "    # actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "    # total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),'accuracy']\n",
    "\n",
    "    # compile keras model with defined optimozer, loss and metrics\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy()\n",
    "    model.compile(optim, loss, metrics)\n",
    "\n",
    "    #dataloader = Dataloder(dataset, batch_size=1, shuffle=False)\n",
    "    for i in range (4):\n",
    "        start_time = time.time()\n",
    "        scores = model.evaluate(x=val_image_set, y=val_annot_set)\n",
    "\n",
    "    print(\"Loss: {:.5}\".format(scores[0]))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #for metric, value in zip(metrics, scores[1:]):\n",
    "    #    print(str(metric)+\"\"+str(value))\n",
    "    #    print(\"mean {}: {:.5}\".format(metric.__name__, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
